{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb8bdf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## <p style=\"font-weight:bold; color:#00747b; font-size:140%; text-align:left;padding: 0; margin: 0; border-bottom: 3px solid #00747b\"> Source Code and Utils Work </p>\n",
    "\n",
    "<div style=\"border-radius:10px; border:#590d0d solid; padding: 15px; background-color: #d4ebea; font-size:100%; text-align:left\">\n",
    "\n",
    "In this we create some utils functions. Based on the idea that we do a monolith notebook they take place in this cell. As we further\n",
    "the notebook the utils functions here will be transfereed to a utlis/src directory as appropriate </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c42631d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import inspect\n",
    "import sklearn\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "notebook_folder = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "destination_path = os.path.join(notebook_folder, \"../data\")\n",
    "results_path= os.path.join(notebook_folder, \"../results\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename= os.path.join(results_path, \"logs.txt\"), force=True)\n",
    "logger = logging.getLogger()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b99a26c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this we create some utils functions. Based on the idea that we do a monolith notebook they take place in this cell. As we further\n",
    "the notebook the utils functions here will be transfereed to a utlis/src directory as appropriate\n",
    "\"\"\"\n",
    "from enum import Enum\n",
    "\n",
    "class Models(Enum):\n",
    "    \"\"\"\n",
    "    Enum class to help with typing and auto-completion in the IDE\n",
    "    \"\"\"\n",
    "    GRADIENT_BOOSTING = 'Gradient Boosting',\n",
    "    GAUSSIAN_NB = 'Gaussian Naive Bayes',\n",
    "    LOG_REGRESSION = 'Logistic Regression',\n",
    "    MLP_CLASSIFIER = 'MLP Classifier',\n",
    "    KNN = 'KNN',\n",
    "    RANDOM_FOREST = 'Random Forest',\n",
    "    DECISION_TREE = 'Decision Tree',\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "210e8563",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Experiment(object):\n",
    "    \"\"\"\n",
    "    A Class to help with running experiments. Also supports using with the \"with\" keyword. The actual functionalities are pretty basic since the core idea is to\n",
    "    be replaced with the mlflow runner\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0\n",
    "        self.model = None\n",
    "        self.results = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = 0\n",
    "        self.start_time = time.time()\n",
    "        self.results = None\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.model is None:\n",
    "            raise Exception(\"You need to run run_experiment(<model/hyperparametertuner>). Possible that there was an error in the\"\n",
    "                            \"fitting of the model\")\n",
    "        self.end_time = 0\n",
    "        self.end_time = time.time()\n",
    "        self.model = None\n",
    "        logger.info(\"Elapsed Time %0.3f\", self.end_time - self.start_time)\n",
    "        logger.info(\"####### End of Experiment ####### \\n\")\n",
    "\n",
    "\n",
    "    def run_experiment(self, experiment_object, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Runs the experiment based on the type of the object\n",
    "        :param experiment_object: Object to be fitted. Can be a model or can be\n",
    "        :param X_train: Training Dataset\n",
    "        :param X_test:  Test Dataset\n",
    "        :param y_train: Training Target\n",
    "        :param y_test: Testing Target\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        timestamp = now.strftime(\"%d%m%y_%H%M\")\n",
    "        logger.info(\n",
    "            \"\\n####### Starting Experiment dated {} #######\\n\".format(\n",
    "                timestamp\n",
    "            )\n",
    "        )\n",
    "        model, metrics_dict = self._get_model_and_results(experiment_object, X_train, X_test, y_train, y_test)\n",
    "        pickle.dump(model, open(os.path.join(results_path ,  timestamp + type(model).__name__ + '.pkl'), 'wb'))\n",
    "\n",
    "\n",
    "    def _get_model_and_results(self, experiment_object, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        Getter method to help with fitting the different experiment objects.\n",
    "        :param experiment_object: Object to be fitted. Can be a model or can be\n",
    "        :param X_train: Training Dataset\n",
    "        :param X_test:  Test Dataset\n",
    "        :param y_train: Training Target\n",
    "        :param y_test: Testing Target\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        ensembles = [x[1] for x in inspect.getmembers(sklearn.ensemble, inspect.isclass)]\n",
    "        tuners = [x[1] for x in inspect.getmembers(sklearn.model_selection)]\n",
    "\n",
    "        if type(experiment_object) in ensembles:\n",
    "            logger.info(\"Model type: %s\", type(experiment_object).__name__)\n",
    "            metrics_dict = {}\n",
    "\n",
    "            logger.debug(\"The experiment object is an ensemble\")\n",
    "            experiment_object.fit(X_train, y_train)\n",
    "            y_pred = experiment_object.predict(X_test)\n",
    "\n",
    "            metrics_dict['accuracy'] = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "            metrics_dict['confusion matrix'] = confusion_matrix(y_test, y_pred)\n",
    "            metrics_dict['classification report'] = classification_report(y_test, y_pred)\n",
    "\n",
    "            logger.debug('The model is %s', self.model)\n",
    "            logger.info(metrics_dict['classification report'])\n",
    "\n",
    "            self.model = experiment_object\n",
    "            return self.model, metrics_dict\n",
    "\n",
    "        elif type(experiment_object) in tuners:\n",
    "            logger.info(\"Hyperparameter tuning experiment\")\n",
    "\n",
    "            metrics_dict = {}\n",
    "            logger.debug(\"The experiment object is a tuner \")\n",
    "            experiment_object.fit(X_train, y_train)\n",
    "\n",
    "            logger.info(\"Model type: %s\", type(experiment_object.best_estimator_).__name__)\n",
    "            self.model = experiment_object.best_estimator_\n",
    "\n",
    "            y_pred = self.model.predict(X_test)\n",
    "\n",
    "            metrics_dict['accuracy'] = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "            metrics_dict['confusion matrix'] = confusion_matrix(y_test, y_pred)\n",
    "            metrics_dict['classification report'] = classification_report(y_test, y_pred)\n",
    "\n",
    "            metrics_dict['best params'] = experiment_object.best_params_\n",
    "            logger.debug('The model is %s', self.model)\n",
    "            logger.info(\"Best parameter (CV score=%0.3f):\", experiment_object.best_score_)\n",
    "            logger.info(\"That following models had the following best parameters %s\", experiment_object.best_params_)\n",
    "\n",
    "            return self.model, metrics_dict\n",
    "        else:\n",
    "            raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6303a179",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "exp = Experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d83ef2",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## <p style=\"font-weight:bold; color:#00747b; font-size:140%; text-align:left;padding: 0; margin: 0; border-bottom: 3px solid #00747b\"> Data Work</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224a4e8-3ba6-4d91-995b-614bd962cc23",
   "metadata": {},
   "source": [
    "####\n",
    "<div style=\"border-radius:10px; border:#590d0d solid; padding: 15px; background-color: #d4ebea; font-size:100%; text-align:left\">\n",
    "    \n",
    "<h2 align=\"center\" style='text-decoration:underline;'> Features in the Dataset</h2>\n",
    "    \n",
    "\n",
    "The dataset contains 12 features and the target variable we are trying to predict. We divide features into buckets based on the forest attributes they are describing. \n",
    "\n",
    "__Features describing the topography of the forest area__ <br>\n",
    "> *Elevation*: The “height of the forest” <br>\n",
    "*Aspect*: The orientation (e.g north-facing) of the slope in degrees (0-360). <br>\n",
    "*Slope*: How steep the area. Measured precent change in elevation over a certain distance. <br>\n",
    "\n",
    "__Features describing distance to water__ <br>\n",
    "> *Horizontal_Distance_To_Hydrology*: Horizontal distance to the nearest water source <br>\n",
    "*Vertical_Distance_To_Hydrology*: Vertical distance to the nearest water source <br>\n",
    "\n",
    "__Features describing the lighting source.__ <br>\n",
    "Here it is important to note that those are related to Elevation, slope and topography <br>\n",
    "> *Hillshade_9am*: Hillshade index at 9. Measured in how bright that part on a grayscale. (1-255) <br>\n",
    "*Hillshade_Noon*: Hillshade index at 12. Measured in how bright that part on a grayscale. (1-255) <br>\n",
    "*Hillshade_3pm*: Hillshade index at 3. Measured in how bright that part on a grayscale. (1-255) <br>\n",
    "\n",
    "__Features describing relevant for fire hazards/urban accessabilty.__ <br>\n",
    "> *Horizontal_Distance_To_Roadways:* Distance to the nearest road. While this generally describes how accessible a forest is, it plays a role in fighting wildfires. <br>\n",
    "*Horizontal_Distance_To_Fire_Points:* Distance to an ignition point in the forest which designates a point where fire is susceptible in a forest. <br>\n",
    "\n",
    "__Area Code__ <br>\n",
    "> *Wilderness_Area:*  The area code to which the current forest area belongs to. <br>\n",
    "\n",
    "__Type of soil present in the described forest area__ <br>\n",
    "> *Soil_Type:* The type of soil in that area of the forest. <br>\n",
    "\n",
    "__Target variable__ <br>\n",
    "> Cover_Type the class to be predicted. The type of forestation cover.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78de74d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/forest_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/raw/forest_data.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mId\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCover_Type\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mcolumns))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pop/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pop/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pop/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pop/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/pop/lib/python3.10/site-packages/pandas/io/common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/raw/forest_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(destination_path, \"raw/forest_data.csv\"), index_col='Id')\n",
    "target = 'Cover_Type'\n",
    "print(len(df.columns))\n",
    "df.hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6fde72-dd59-4f6e-910d-f69d803bb27a",
   "metadata": {},
   "source": [
    "####\n",
    "<div style=\"border-radius:10px; border:#590d0d solid; padding: 15px; background-color: #d4ebea; font-size:100%; text-align:left\">\n",
    "    \n",
    "<h2 align=\"center\" style='text-decoration:underline;'> Correlation Analysis</h2>\n",
    "\n",
    "When looking at the correlation between the Cover type (our target) and the rest of the features in the dataset there are no strong correlation we can find. The highest one is a positive one with Wilderness Area, however with only 0.2. \n",
    "\n",
    "We have some other interesting correlations down to note (non-exhaustive), __but remember to note that any conclusions made out of the correlation are just hypothesis that might need validation.__\n",
    "\n",
    "- *HIllshade_9am and HIllshade_3pm:* High negative correlation of -0.78. Sounds logical if you think of the sun trajectory above a hill over the course of the day.\n",
    "- *Hillshade_Noon and Hillshade_3pm:* Positive Correlation of 0.66.\n",
    "- *Aspect and Hillshade_9am / Aspect and Hillshade_3pm:* With the former being -0.59 and the latter being 0.63 they show that the Hillshade_9am and the Hillshade_3pm have a very similar but inverse relationship to Aspect. Again, thinking of a sun’s trajectory over a hill and the Aspect describing which direction the point is facing (e.g east-facing/west-facing), it makes sense.\n",
    "- *Soil_type and Elevation:* The highest correlation with 0.83. The reason for that is not clear for now, however this is where Subject Matter Experts are important! At this point you should start thinking about contacting them to help describe why is there a strong relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98560e5-61d0-4d62-9a13-74077985260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = df.copy()\n",
    "corr_matrix = attributes.corr()\n",
    "corr_matrix['Elevation'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884f439-48c0-44f8-b956-cd3ade287a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(attributes.corr(), annot=True, fmt='.1g', vmin=-1, vmax=1, center=0, cmap=cmap)\n",
    "plt.title(\"Correlation Matrix\", fontweight='bold', fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f00f28-9237-479f-a603-1c8e9a730030",
   "metadata": {},
   "source": [
    "####\n",
    "<div style=\"border-radius:10px; border:#590d0d solid; padding: 15px; background-color: #d4ebea; font-size:100%; text-align:left\">\n",
    "    \n",
    "<h2 align=\"center\" style='text-decoration:underline;'> Training Data split </h2>\n",
    "\n",
    "We check if the data classes are balanced. With exactly 2160 per class, The dataset is perfectly balanced and no further work is required. We create a random split with a fixed seed to be always able to reproduce the specfic split. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda8915",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[target].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1d4db-9c03-4552-99e5-801bfde00d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "y = df[target]\n",
    "X = df.drop(target, axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263103a-6725-472a-8db0-caf413391f9d",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "<div style=\"border-radius:10px; border:#590d0d solid; padding: 15px; background-color: #d4ebea; font-size:100%; text-align:left\">\n",
    "    \n",
    "<h2 align=\"center\" style='text-decoration:underline;'> Feature Engineering </h2>\n",
    "\n",
    "We apply basic feature engineering to the dataset where we <br>\n",
    "\n",
    "1. __Apply One Hot Encoding__ : Although our categorical features ('Wilderness_Area' and 'Soil_Type') are already integer encoded, integer encoding might imply an ordinal relationship. This is not the case here and therefore we apply One Hot Encoding\n",
    "2. __Scale the numerical features__: As we know from looking at our attributes they have very different scales. Having all the data on the scale is important to avoid scenarios where one feature would have more impact only due to the different scale magnifying its effect\n",
    "\n",
    "Other ideas that coule have been done are __Calculating the Euclidean Distance to the hydrology__ or __assigning a fire hazard score based on the distance to the road and distance to the ignition point__ \n",
    "\n",
    "Finally we create a sperate training/test dataset with the changes to compare the effects later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be588500-9ad1-4e0c-be9d-9b2ba674491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Wilderness_Area', 'Soil_Type']\n",
    "num_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "       'Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "col_transformer = make_column_transformer(\n",
    "        (OneHotEncoder(), cat_cols),\n",
    "        remainder=StandardScaler())\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f45f04-8931-47d1-bb30-694e61633132",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = col_transformer.fit_transform(X_train)\n",
    "X_test_transformed = col_transformer.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
